{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-36600ca280b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_ridge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKernelRidge\u001b[0m\u001b[0;31m#3from catboost import Pool, CatBoostRegressor, cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pds.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from statistics import mode\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.preprocessing import LabelEncoder,RobustScaler\n",
    "from sklearn.linear_model import Ridge, Lasso,ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge#3from catboost import Pool, CatBoostRegressor, cv\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../input/train.csv\")\n",
    "test=pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "835af1a380a280287ef2030fe95aa1df4774f0f9"
   },
   "outputs": [],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0e07ca1d3a4fa81ca173f248db62101de2870e0"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b103a88373444b8f9ecc51ec3875d626aa2b096"
   },
   "source": [
    "now looking at the target variable, we donnot need the ID as it is not usefull for the model prediction we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06780ac8d94fa42e375e964b02376938bf492e6f"
   },
   "outputs": [],
   "source": [
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "# Now drop the 'Id' colum since it's unnecessary for the prediction process\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9361b15c7466102ec8eb85e85ae5f9141e819dd2"
   },
   "outputs": [],
   "source": [
    "sns.regplot(train[\"GrLivArea\"],y=train[\"SalePrice\"],fit_reg=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df16801f83e6b7872f6e77518674278189a76aee"
   },
   "source": [
    "we can see there are few outliers in the dataset we can go ahead and remove them as these would affect over model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b8a70d9a9ac99510310d7eb577c05909e02bb2d"
   },
   "outputs": [],
   "source": [
    "# Removing two very extreme outliers in the bottom right hand corner\n",
    "train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "\n",
    "# Re-check graph\n",
    "sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2e64ca077e6656bb9a80c1e364bb07bec15545a"
   },
   "source": [
    "now taking a look at the target values for any kind of skewness  present in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e34dda95bfe141ad386e032647759f66d8bbbbb"
   },
   "outputs": [],
   "source": [
    "(mu,sigma)=norm.fit(train.SalePrice)\n",
    "sns.distplot(train.SalePrice,fit=norm)\n",
    "plt.legend([\"$\\mu=$ {:.2f} and $\\sigma=$ {:.2f}\".format(mu,sigma)],loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba42dbf204e1edcdf3f64c66fd37c452425a22cb"
   },
   "source": [
    "From the above plot we can see that the data is bit right skewed. we can make it to normal by applying the log for the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d5dc63922aaffe085b298935fa4d0b3a62db533"
   },
   "outputs": [],
   "source": [
    "train.SalePrice = np.log1p(train.SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61c051c35921581efb771ef31c30565acd576406"
   },
   "outputs": [],
   "source": [
    "(mu,sigma)=norm.fit(train.SalePrice)\n",
    "sns.distplot(train.SalePrice,fit=norm)\n",
    "plt.legend([\"$\\mu=$ {:.2f} and $\\sigma=$ {:.2f}\".format(mu,sigma)],loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f37c166d00bbd9701bc8fd8f3a01b7503af1de0a"
   },
   "source": [
    "First we will make the target values seperate from the Training values and combine th train and test sets to make all the work together rather doing seperate for each once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2551ed9d8f5a014f8b7cbafcf0dd60d284b4c2be"
   },
   "outputs": [],
   "source": [
    "train_nS=train.shape[0]\n",
    "test_nS=test.shape[0] # shpaes of train and tests for sperating them back\n",
    "\n",
    "train_y=train.SalePrice.values\n",
    "full_data=pd.concat((train,test)).reset_index(drop=True) #concating the train and test sets\n",
    "\n",
    "full_data.drop([\"SalePrice\"],axis=1,inplace=True) #dropping the target values\n",
    "\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ddf6284058da3ad50754bdeda8d89bb4064bd81"
   },
   "source": [
    "Now our target values looks well distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8deb127c7d9626ecb2aa750dc8c13b557242fa50"
   },
   "source": [
    "Now looking at the missing values and imputing with the appropriate values. Now will  find the percenatge of missing values in each features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81f3be7c752ded14cd445a726241930435c6f6fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_data_rank=(full_data.isnull().sum()/len(full_data))*100\n",
    "print(\"total number of columns with values misiing : {}\".format(missing_data_rank[missing_data_rank>0].count()))\n",
    "missed =pd.DataFrame({\"Missing Percentage\": missing_data_rank[missing_data_rank>0].sort_values(ascending =False)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74b45f05e1b4fe12db7dbfdb957d431877801e88"
   },
   "source": [
    "We can see that only few features have high rank of missing values in them and rest are prety good enough with less than 10%.\n",
    "Now lets dive into data and look how can we fill up this missed values: there are two ways in deeling the missing values:\n",
    "1. We can drop the row for the values misisng in them, tough this is not the ideal choice if we havel less traning data and droping data can lead the model to ineffecinet.\n",
    "2. Than droping the row and loosing the data, we can fill up or impute the mising values with the appropriate values like using mean, median and mode values.\n",
    "So am going to keep the missing rows and fill them with appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "288fb9034924b643fb22bd748dfc4c652d626fb5"
   },
   "outputs": [],
   "source": [
    "missed_features=list(missed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9991fb15f90cf448003f807538e5a470e0428a7"
   },
   "outputs": [],
   "source": [
    "full_data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18716946ad8b23d42fbb7eb02feb80c07457c0d5"
   },
   "outputs": [],
   "source": [
    "full_data.GarageQual.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e0ffff2324d695edd5b5b4fb09c88eb06c44402"
   },
   "outputs": [],
   "source": [
    "# All columns where missing values can be replaced with 'None'\n",
    "for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n",
    "    full_data[col] = full_data[col].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe302b1ef8d9c0327e3c14675276432b4a133d60"
   },
   "outputs": [],
   "source": [
    "\n",
    "# All columns where missing values can be replaced with 0\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n",
    "    full_data[col] = full_data[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0bfc5d147e6d41773d2c3f009ab1c48cddd16e4"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# All columns where missing values can be replaced with the mode (most frequently occurring value)\n",
    "for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n",
    "    full_data[col] = full_data[col].fillna(full_data[col].mode()[0])\n",
    "\n",
    "# Imputing LotFrontage with the median (middle) value\n",
    "full_data['LotFrontage'] = full_data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "656bf1dae5544329fedfe109435aa0bc86855285"
   },
   "outputs": [],
   "source": [
    "full_data['TotalSF'] = full_data['TotalBsmtSF'] + full_data['1stFlrSF'] + full_data['2ndFlrSF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53b308fb7eef7acb2b0157ade5400c2dde41523d"
   },
   "outputs": [],
   "source": [
    "missing_data=(full_data.isnull().sum()/len(full_data))*100\n",
    "print(\"total number of columns with values misiing : {}\".format(missing_data[missing_data>0].count()))\n",
    "missed =pd.DataFrame({\"Missing Percentage\": missing_data[missing_data>0].sort_values(ascending =False)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f73f02d15e2b48190e702ec266934878c0029721"
   },
   "source": [
    "Now we cleaned up our missing values and its time to move to the categorical values as our models can work with only numeric data, now we change our categorical values to numeric by labeling them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f973b558657f45a2c8fbaf623c726000bc8a5e0f"
   },
   "outputs": [],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d60184dcf7e586940b36367499b6f561a03bf64"
   },
   "source": [
    "I thing few of the features should be categorical but are as numeric in data, so I am going to change them to categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbab0a0c2b7b1a1d74d200db18c050cba1a9affc"
   },
   "outputs": [],
   "source": [
    "# Converting those variables which should be categorical, rather than numeric\n",
    "for col in ('MSSubClass', 'OverallCond', 'YrSold', 'MoSold'):\n",
    "    full_data[col] = full_data[col].astype(str)\n",
    "    \n",
    "full_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72a7d2acee4b85d5046aa2a4e00bbe3fefc0de3d"
   },
   "source": [
    "As we alredy change the right skewed target values to normal distribution, now i am going to cange alll the numeric data to normal distribution if any data is skewd internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bbedac4dfdc521358ebc41b932fe4ef271f6377"
   },
   "outputs": [],
   "source": [
    "# Applying a log(1+x) transformation to all skewed numeric features\n",
    "numeric_feats = full_data.dtypes[full_data.dtypes != \"object\"].index\n",
    "\n",
    "# Compute skewness\n",
    "skewed_feats = full_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a81560c17eaa88f97fc8a6fe5ba16250e5a5add6"
   },
   "source": [
    "\n",
    "\n",
    "Box Cox Transformation of (highly) skewed features\n",
    "\n",
    "Skewed features are a formality when dealing with real-world data. Transformation techniques can help to stabilize variance, make data more normal distribution-like and improve the validity of measures of association.\n",
    "\n",
    "The problem with the Box-Cox Transformation is estimating lambda. This value will depend on the existing data, and as such should be considered when performing cross validation on out of sample datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f25e4ea9a72fd447ef8fc526361b4634ef45cac"
   },
   "outputs": [],
   "source": [
    "# Check on number of skewed features above 75% threshold\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\"Total number of features requiring a fix for skewness is: {}\".format(skewness.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "370a3900da00e91aabc4703015c63085d63b2d61"
   },
   "outputs": [],
   "source": [
    "# Now let's apply the box-cox transformation to correct for skewness\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feature in skewed_features:\n",
    "    full_data[feature] = boxcox1p(full_data[feature], lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ed48126e35eff7b692e7c1f4778dc470812484b"
   },
   "source": [
    "Now find any features which is highly represented i.e the values in the features are same to the  extend of 97%, this values donot play any role in the model prediction so we will drop those features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0d8ed1a46c4f40e13a029f448339dd0184bead4"
   },
   "outputs": [],
   "source": [
    "full_data = full_data.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c345422ae6976fa6909fc2063a80872d1a265acf"
   },
   "outputs": [],
   "source": [
    "#highlyrepeated_values= [col for col in full_data.select_dtypes(exclude=['number']) if 1 - sum(full_data[col] == mode(full_data[col]))/len(full_data) < 0.03]\n",
    "# Dropping these columns from both datasets\n",
    "#full_data = full_data.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75a5638efc58c6bf8ea7c21076bc728a4054f561"
   },
   "source": [
    "\n",
    "Label encoding\n",
    "\n",
    "This step build on the previous step whereby all text data will become numeric. This is a requirement for Machine Learning, that is, only numerical data can be fed into a predictive model. There are many other encoding techniques available, some of which more powerful than Label Encoding which does incur the risk of falsely ranking variables, e.g. coding three locations into 0, 1 and 2 might imply that 2 is a higher value than 0, which is incorrect as the numbers just represent different categories (locations). This is a simple approach, however, and therefore I'm going to stick with it for the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a0badd9d41fa657d20081bdba8acd0c4e1fd87f"
   },
   "outputs": [],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce7cdc92f3960ba6d86149481ab72747090812bd"
   },
   "outputs": [],
   "source": [
    "obj_features=list(full_data.select_dtypes(include=\"object\").columns)\n",
    "len(obj_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa74aae9de2f6025b9514c4a19e9b2d3dd7c8a1d"
   },
   "outputs": [],
   "source": [
    "# List of columns to Label Encode\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "\n",
    "# Process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(full_data[c].values)) \n",
    "    full_data[c] = lbl.transform(list(full_data[c].values))\n",
    "\n",
    "# Check on data shape        \n",
    "print('Shape all_data: {}'.format(full_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d90e7d8f32f6835214949db4940c710bcf7f90fa"
   },
   "outputs": [],
   "source": [
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccad0a59729c4cd5788e9caec649c192ea1b80e4"
   },
   "outputs": [],
   "source": [
    "full_data=pd.get_dummies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad2fb442625edd5f4953da89773eb2de5496b638"
   },
   "outputs": [],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b97b20cb8a7430b15ac1bbb01d5ae9626d98623b"
   },
   "outputs": [],
   "source": [
    "full_data.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83a221187ec649f8666b85ee244fd92bdc60239b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now to return to separate train/test sets for Machine Learning\n",
    "train_x = full_data[:train_nS]\n",
    "test_x= full_data[train_nS:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8bf928335b30dbe10f3faa1b3ee077a8357fe849"
   },
   "outputs": [],
   "source": [
    "# Defining two rmse_cv functions\n",
    "\n",
    "def rmse_cv(model):\n",
    "    \n",
    "    rmse=np.sqrt(-cross_val_score(model, train_x,train_y,scoring=\"neg_mean_squared_error\",cv=10))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c33cb5be01b2b4544bb03c54df000c605f430ab5"
   },
   "source": [
    "Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58054edafef16d071f17621106f1d921e96c946b"
   },
   "outputs": [],
   "source": [
    "\n",
    "alphas = [0.05, 0.1, 0.3, 1, 3, 5,7, 10, 15, 30]\n",
    "#alphas=np.arange(0.05,30,0.05)\n",
    "# Iterate over alpha's\n",
    "cv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4107d037d2fcd98eeee575d1dd6b6704d99beec"
   },
   "outputs": [],
   "source": [
    "print(cv_ridge)\n",
    "cv_ridge = pd.Series(cv_ridge, index = alphas)\n",
    "cv_ridge.plot(title = \"Validation\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e267ff08cde895198309789de3c1184e53c49e9f"
   },
   "outputs": [],
   "source": [
    "# 5 looks like the optimal alpha level, so let's fit the Ridge model with this value\n",
    "#model_ridge = Ridge(alpha = 10)\n",
    "model_ridge = Ridge(alpha = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7a151ebcdddf603e31cfcff3627ec2a8cc42ddb"
   },
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.005, 0.001, 0.0002,0.0003,0.0004,0.0005,0.0001]\n",
    "#alphas=np.arange(0.0001,0.01,0.0005)\n",
    "# Iterate over alpha's\n",
    "cv_lasso = [rmse_cv(Lasso(alpha = alpha,random_state=1)).mean() for alpha in alphas]\n",
    "\n",
    "# Plot findings\n",
    "cv_lasso = pd.Series(cv_lasso, index = alphas)\n",
    "cv_lasso.plot(title = \"Validation\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Rmse\")\n",
    "print(cv_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71a18277443979d0b719f680eef23a83b51ce56f"
   },
   "outputs": [],
   "source": [
    "# Initiating Lasso model\n",
    "model_lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0004))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e39411828eb511425d6d6eae1359fc48f639ec8b"
   },
   "source": [
    "ElasticNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a8be3e15583b6893fccbcd53dd8e27a53c8ee84"
   },
   "outputs": [],
   "source": [
    "# Setting up list of alpha's\n",
    "alphas = [0.01, 0.005, 0.001, 0.00055,0.0006, 0.0001]\n",
    "#alphas=np.arange(0.0001,1,0.0004)\n",
    "# Iterate over alpha's\n",
    "cv_elastic = [rmse_cv(ElasticNet(alpha = alpha)).mean() for alpha in alphas]\n",
    "\n",
    "# Plot findings\n",
    "cv_elastic = pd.Series(cv_elastic, index = alphas)\n",
    "cv_elastic.plot(title = \"Validation\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Rmse\")\n",
    "print(cv_elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e869fb82e86160b5000db6f4bdc56d7e7040191c"
   },
   "outputs": [],
   "source": [
    "# Initiating ElasticNet model\n",
    "model_elasticnet = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.0006))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79c55ad10d68451bf43453fd4e2178ecc40189ab"
   },
   "source": [
    "\n",
    " 4. Kernel ridge regression\n",
    "\n",
    "OK, this is not strictly a generalized linear model. Kernel ridge regression (KRR) combines Ridge Regression (linear least squares with l2-norm regularization) with the 'kernel trick'. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bd450e8ed2d9732f753fc32ce0cab2ddf28e60f"
   },
   "outputs": [],
   "source": [
    "# Setting up list of alpha's\n",
    "alphas = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "\n",
    "# Iterate over alpha's\n",
    "cv_krr = [rmse_cv(KernelRidge(alpha = alpha)).mean() for alpha in alphas]\n",
    "\n",
    "# Plot findings\n",
    "cv_krr = pd.Series(cv_krr, index = alphas)\n",
    "cv_krr.plot(title = \"Validation\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Rmse\")\n",
    "print(cv_krr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b720ffb2b5088e7a0c21e5319bc6efe6a44682a"
   },
   "outputs": [],
   "source": [
    "# Initiatiing KernelRidge model\n",
    "model_krr = make_pipeline(RobustScaler(), KernelRidge(alpha=7, kernel='polynomial', degree=2.65, coef0=6.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df00632ede453adfecfe38185c1e9577233d186d"
   },
   "source": [
    "\n",
    "B. Ensemble methods (Gradient tree boosting)\n",
    "\n",
    "Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n",
    "\n",
    "This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of a boosting algorithm, and these are what i'll be applying to the current data next.\n",
    " 5. Gradient Boosting\n",
    "\n",
    "For the Gradient Boosting algorithm I will use 'huber' as the loss function as this is robust to outliers. The other parameters on display originate from other kernels tackling this challenge, followed by trial and error to refine them to this specific dataset. Again, applying GridSearchCV will help to define a better set of parameters than those currently on display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "138dfb5516e2d083b71bf4e371a05c21ef7213e8"
   },
   "outputs": [],
   "source": [
    "# Initiating Gradient Boosting Regressor\n",
    "model_gbr = GradientBoostingRegressor(n_estimators=1200, \n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=4, \n",
    "                                      max_features='sqrt',\n",
    "                                      min_samples_leaf=15, \n",
    "                                      min_samples_split=10, \n",
    "                                      loss='huber',\n",
    "                                      random_state=5)\n",
    "cv_gbr=rmse_cv(model_gbr).mean()\n",
    "cv_gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45359e2a603d73800853dcaa4a3c43afa2438781"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initiating XGBRegressor\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.2,\n",
    "                             learning_rate=0.025,\n",
    "                             max_depth=3,\n",
    "                             n_estimators=1550)\n",
    "cv_xgb = rmse_cv(model_xgb).mean()\n",
    "cv_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4b2c6d710dd9c69afcb3c6ecd049a2f7d7803e7"
   },
   "outputs": [],
   "source": [
    "# Initiating LGBMRegressor model\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',\n",
    "                              num_leaves=4,\n",
    "                              learning_rate=0.05, \n",
    "                              n_estimators=1080,\n",
    "                              max_bin=75, \n",
    "                              bagging_fraction=0.80,\n",
    "                              bagging_freq=5, \n",
    "                              feature_fraction=0.232,\n",
    "                              feature_fraction_seed=9, \n",
    "                              bagging_seed=9,\n",
    "                              min_data_in_leaf=6, \n",
    "                              min_sum_hessian_in_leaf=11)\n",
    "cv_lgb = rmse_cv(model_lgb).mean()\n",
    "cv_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84e65a1ad8c6ecb7118b312e5021c4c6d8842a81"
   },
   "outputs": [],
   "source": [
    "# Fitting all models with rmse_cv function, apart from CatBoost\n",
    "cv_ridge = rmse_cv(model_ridge).mean()\n",
    "cv_lasso = rmse_cv(model_lasso).mean()\n",
    "cv_elastic = rmse_cv(model_elasticnet).mean()\n",
    "cv_krr = rmse_cv(model_krr).mean()\n",
    "cv_gbr = rmse_cv(model_gbr).mean()\n",
    "cv_xgb = rmse_cv(model_xgb).mean()\n",
    "cv_lgb = rmse_cv(model_lgb).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9b23c168975f2ea4fc3097c2cf5f6a89d546f86"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating a table of results, ranked highest to lowest\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Ridge',\n",
    "              'Lasso',\n",
    "              'ElasticNet',\n",
    "              'Kernel Ridge',\n",
    "              'Gradient Boosting Regressor',\n",
    "              'XGBoost Regressor',\n",
    "              'Light Gradient Boosting Regressor',\n",
    "              ],\n",
    "    'Score': [cv_ridge,\n",
    "              cv_lasso,\n",
    "              cv_elastic,\n",
    "              cv_krr,\n",
    "              cv_gbr,\n",
    "              cv_xgb,\n",
    "              cv_lgb]})\n",
    "\n",
    "# Build dataframe of values\n",
    "result_df = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\n",
    "result_df.head(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0382bc1971b78140b8aa080d795b9ee8fba00772"
   },
   "source": [
    "\n",
    "3. Stacking algorithms\n",
    "\n",
    "I've ran eight models thus far, and they've all performed pretty well. I'm now quite keen to explore stacking as a means of achieving an even higher score. In a nutshell, stacking uses as a first-level (base) the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions. Stacking can be beneficial as combining models allows the best elements of their predictive power on the given challenged to be pooled, thus smoothing over any gaps left from an individual model and increasing the likelihood of stronger overall model performance.\n",
    "\n",
    "Ok, let's get model predictions and then stack the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a6014a01eeb0585949605b601528ee21ced6ce3"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c49d2643625cf405675b2f4edb4965522eee3945"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Averaged base models score\n",
    "averaged_models = AveragingModels(models = (model_elasticnet, model_gbr, model_krr, model_lasso))\n",
    "score = rmse_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9144dfe9a4b08499920e87a842dbede005d7b60"
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    #kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train_x.values, train_y, scoring=\"neg_mean_squared_error\", cv = 10))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c689139a3f4c1229a6aad02e2d89df34b1f3a0e6"
   },
   "outputs": [],
   "source": [
    "#Stacking averaged Models Class\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e761df6da0626438e8ce878d9f641ebd1619ce6c"
   },
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (model_elasticnet, model_gbr, model_krr),meta_model = model_lasso)\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b212cc4fa5e8dce0f90313271bbb0701ea68a195"
   },
   "outputs": [],
   "source": [
    "#define a rmsle evaluation function\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b4083cccb944f0d8cbb4c3f278daae9e851402c"
   },
   "outputs": [],
   "source": [
    "#Final Training and Prediction\n",
    "stacked_averaged_models.fit(train_x.values, train_y)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train_x.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test_x.values))\n",
    "print(rmsle(train_y, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb410271e5d09c9028b73ab2835842f94fdf563a"
   },
   "source": [
    "**XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15351950222bb81d01b8886c39dab26c8139dc10"
   },
   "outputs": [],
   "source": [
    "model_xgb.fit(train_x, train_y)\n",
    "xgb_train_pred = model_xgb.predict(train_x)\n",
    "xgb_pred = np.expm1(model_xgb.predict(test_x))\n",
    "print(rmsle(train_y, xgb_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76c6a3e91f7bf3b4fe97ef8d1ddc5cd24dfa875b"
   },
   "source": [
    "**LGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57f448daf3c61f3552b76dc4d2fd23f07f42c7b1"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_lgb.fit(train_x, train_y)\n",
    "lgb_train_pred = model_lgb.predict(train_x)\n",
    "lgb_pred = np.expm1(model_lgb.predict(test_x.values))\n",
    "print(rmsle(train_y, lgb_train_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ca004327214a8480a6677e8e9bc6f27c8d3764e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('RMSLE score on train data:')\n",
    "print(rmsle(train_y,stacked_train_pred*0.70 +\n",
    "               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e42f24d9ed111c8b92a3552c698d035c68465b7e"
   },
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n",
    "#ensemble=xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5885177f7a0ec5feb4b26cf8352515028dde215"
   },
   "outputs": [],
   "source": [
    "# Create stacked model\n",
    "#stacked = (lasso_pred + elastic_pred + ridge_pred + xgb_pred + lgb_pred + krr_pred + gbr_pred) / 7\n",
    "# Setting up competition submission\n",
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test_ID\n",
    "sub['SalePrice'] = ensemble #stacked\n",
    "sub.to_csv('house_price_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6c65b61078ca33eb87fa141e9582c6563f746b2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
